{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/7cKHhOO/34B2aPVkrMge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WoutSin/Intensive_Course_II/blob/main/Runic_Emendation_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import and run relevant functions**\n",
        "\n",
        "Run the code below by clicking on the play button to load the functions"
      ],
      "metadata": {
        "id": "OfsBgZwZTsPi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9x--ajhTOHh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_runestones(file_path):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        file_path: path to a .txt file containing runic inscriptions\n",
        "    Output:\n",
        "        lines: list of runic inscriptions\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = [line.strip() for line in file.readlines()]\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_excel_to_dataframe(file_path):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        file_path: path to an Excel file containing metadata on runic inscriptions\n",
        "    Output:\n",
        "        df: dataframe containing metadata on runic inscriptions\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(file_path)\n",
        "    return df\n",
        "\n",
        "\n",
        "def filter_dataframe(df):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        df: dataframe containing metadata on the runic inscriptions\n",
        "    Output:\n",
        "        df: dataframe containing metadata on runic inscriptions from the Medieval and Viking Age period\n",
        "    \"\"\"\n",
        "    df[\"Period/Datering\"] = df[\"Period/Datering\"].fillna(\"NaN\")\n",
        "    df = df[\n",
        "        df[\"Period/Datering\"].str.startswith(\"M\")\n",
        "        | df[\"Period/Datering\"].str.startswith(\"V\")\n",
        "    ]\n",
        "    return df\n",
        "\n",
        "\n",
        "def filter_inscriptions(df, inscriptions):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        df: dataframe containing metadata on the runic inscriptions\n",
        "        inscriptions: list of runic inscriptions\n",
        "    Output:\n",
        "        filtered_inscriptions: selection of Medieval and Viking Age runic inscriptions\n",
        "    \"\"\"\n",
        "    filtered_inscriptions = []\n",
        "    for signum in df[\"Signum\"]:\n",
        "        for inscription in inscriptions:\n",
        "            if inscription.startswith(signum):\n",
        "                filtered_inscription = inscription[len(signum) :].strip()\n",
        "                filtered_inscriptions.append(filtered_inscription)\n",
        "\n",
        "    filtered_inscription = list(set(filtered_inscriptions))\n",
        "\n",
        "    return filtered_inscriptions\n",
        "\n",
        "\n",
        "def remove_punctuation(input_string, punctuation):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        input_string: runic sequence (string)\n",
        "        punctuation: string of non-alpabetical characters to be removed\n",
        "    Output:\n",
        "        no_punct: runic sequence without unnecessary punctuation (string)\n",
        "    \"\"\"\n",
        "    translator = str.maketrans(\"\", \"\", punctuation)\n",
        "    no_punct = input_string.translate(translator)\n",
        "\n",
        "    return no_punct\n",
        "\n",
        "\n",
        "def print_non_alpha(runic_list):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        runic_list: list of runic inscriptions\n",
        "    Output:\n",
        "        non_alpha_chars: string of non-alpabetical characters to be removed\n",
        "    \"\"\"\n",
        "    non_alpha_chars = set()\n",
        "    for item in runic_list:\n",
        "        non_alpha_chars.update(\n",
        "            [\n",
        "                char\n",
        "                for char in item\n",
        "                if not char.isalnum() and char not in [\"-\", \"…\", \" \"]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    return \"\".join(non_alpha_chars)\n",
        "\n",
        "\n",
        "def tokenize_text(input_string):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "       input_string: runic sequence (string)\n",
        "    Output:\n",
        "       list of tokens in the runic sequence\n",
        "    \"\"\"\n",
        "    return [token for token in input_string.split(\" \") if token]\n",
        "\n",
        "\n",
        "def get_tags(runic_list):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        runic_list: tokenized runic sequence\n",
        "    Output:\n",
        "        output_list: tuple containing tokens and their respective tags\n",
        "    Tags:\n",
        "        <com>: complete tokens, no characters are missing\n",
        "        <inc>: incomplete tokens, one or multiple characters are missing\n",
        "        <mis>: missing tokens, all characters are missing\n",
        "    Characters:\n",
        "        - denotes a one missing character\n",
        "        … denotes one or more missing characters\n",
        "        Example: \"stein\", \"st--n\", \"st…n\"\n",
        "    \"\"\"\n",
        "    special_chars = [\"-\", \"…\"]\n",
        "    output_list = []\n",
        "\n",
        "    for item in runic_list:\n",
        "        if any(char in item for char in special_chars) and any(\n",
        "            char not in special_chars for char in item\n",
        "        ):\n",
        "            output_list.append((item, \"<inc>\"))\n",
        "        elif all(char in special_chars for char in item):\n",
        "            output_list.append((item, \"<mis>\"))\n",
        "        else:\n",
        "            output_list.append((item, \"<com>\"))\n",
        "\n",
        "    return output_list\n",
        "\n",
        "\n",
        "def split_data(tagged_sequences):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        tagged_sequences: list containing runic sequences as (token, tag) tuples\n",
        "    Output:\n",
        "        train: selection of train data with size sequences - 100\n",
        "        test: selection of test data with size 100\n",
        "    \"\"\"\n",
        "    train, test = train_test_split(tagged_sequences, test_size=0.05, random_state=27)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def extract_ngram_probabilities(tagged_sequences):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        tagged_sequences: list containing runic sequences as (token, tag) tuples\n",
        "    Output:\n",
        "        unigrams: dictionary with key = one token and value = probability of token\n",
        "        bigrams: dictionary with key = two tokens and value = probability of token 2 given token 1\n",
        "        trigrams: dictionary with key = three tokens and value = probability of token 3 given token 1 and token 2\n",
        "    \"\"\"\n",
        "    unigrams = defaultdict(int)\n",
        "    bigrams = defaultdict(Counter)\n",
        "    trigrams = defaultdict(Counter)\n",
        "\n",
        "    for tagged_sequence in tagged_sequences:\n",
        "        com_items = []\n",
        "\n",
        "        for i in range(len(tagged_sequence)):\n",
        "            if tagged_sequence[i][1] == \"<com>\":\n",
        "                com_items.append(tagged_sequence[i][0])\n",
        "                unigrams[tagged_sequence[i][0]] += 1\n",
        "            else:\n",
        "                com_items.append(None)\n",
        "\n",
        "        for i in range(len(com_items)):\n",
        "            if (\n",
        "                i < len(com_items) - 1\n",
        "                and com_items[i] is not None\n",
        "                and com_items[i + 1] is not None\n",
        "            ):\n",
        "                bigrams[com_items[i]][com_items[i + 1]] += 1\n",
        "            if (\n",
        "                i < len(com_items) - 2\n",
        "                and com_items[i] is not None\n",
        "                and com_items[i + 1] is not None\n",
        "                and com_items[i + 2] is not None\n",
        "            ):\n",
        "                trigrams[(com_items[i], com_items[i + 1])][com_items[i + 2]] += 1\n",
        "\n",
        "    total_unigrams = sum(unigrams.values())\n",
        "    for word in unigrams:\n",
        "        unigrams[word] /= total_unigrams\n",
        "\n",
        "    for word in bigrams:\n",
        "        total_count = sum(bigrams[word].values())\n",
        "        for next_word in bigrams[word]:\n",
        "            bigrams[word][next_word] /= total_count\n",
        "\n",
        "    for words in trigrams:\n",
        "        total_count = sum(trigrams[words].values())\n",
        "        for next_word in trigrams[words]:\n",
        "            trigrams[words][next_word] /= total_count\n",
        "\n",
        "    return unigrams, bigrams, trigrams\n",
        "\n",
        "\n",
        "def save_probabilities_to_file(unigrams, bigrams, trigrams, filename):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        unigrams: dictionary with key = one token and value = probability of token\n",
        "        bigrams: dictionary with key = two tokens and value = probability of token 2 given token 1\n",
        "        trigrams: dictionary with key = three tokens and value = probability of token 3 given token 1 and token 2\n",
        "        filename: name of the file to which the dictionaries should be saved\n",
        "    Output:\n",
        "        .txt file containing the n-gram probabilities\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"Unigram Probabilities:\\n\")\n",
        "        for word, probability in unigrams.items():\n",
        "            f.write(f\"{word}: {probability}\\n\")\n",
        "\n",
        "        f.write(\"\\nBigram Probabilities:\\n\")\n",
        "        for word, next_words in bigrams.items():\n",
        "            for next_word, probability in next_words.items():\n",
        "                f.write(f\"{word} {next_word}: {probability}\\n\")\n",
        "\n",
        "        f.write(\"\\nTrigram Probabilities:\\n\")\n",
        "        for words, next_words in trigrams.items():\n",
        "            for next_word, probability in next_words.items():\n",
        "                f.write(f\"{words[0]} {words[1]} {next_word}: {probability}\\n\")\n",
        "\n",
        "\n",
        "def extract_unigram_tokens(unigrams_dictionary):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        unigrams_dictionary: dictionary with key = one token and value = probability of token\n",
        "    Output:\n",
        "        unigram_words: list containing (unigram, probability) tuples\n",
        "    \"\"\"\n",
        "    unigram_words = []\n",
        "\n",
        "    for word, probability in unigrams_dictionary.items():\n",
        "        unigram_words.append((word, probability))\n",
        "\n",
        "    return list(set(unigram_words))\n",
        "\n",
        "\n",
        "def extract_bigram_tokens(bigrams_dictionary):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        bigrams: dictionary with key = two tokens and value = probability of token 2 given token 1\n",
        "    Output:\n",
        "        bigram_words: list containing (bigram, probability) tuples\n",
        "    \"\"\"\n",
        "    bigram_words = []\n",
        "\n",
        "    for word, next_words in bigrams_dictionary.items():\n",
        "        for next_word, probability in next_words.items():\n",
        "            bigram_words.append(((word, next_word), probability))\n",
        "\n",
        "    return list(set(bigram_words))\n",
        "\n",
        "\n",
        "def extract_trigram_tokens(trigrams_dictionary):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        trigrams: dictionary with key = three tokens and value = probability of token 3 given token 1 and token 2\n",
        "    Output:\n",
        "        trigram_words: list containing (trigram, probability) tuples\n",
        "    \"\"\"\n",
        "    trigram_words = []\n",
        "\n",
        "    for words, next_words in trigrams_dictionary.items():\n",
        "        for next_word, probability in next_words.items():\n",
        "            trigram_words.append(((words[0], words[1], next_word), probability))\n",
        "\n",
        "    return list(set(trigram_words))\n",
        "\n",
        "\n",
        "def extract_potential_tokens(tagged_sequences, unigrams, bigrams, trigrams):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        tagged_sequences: list containing runic sequences as (token, tag) tuples\n",
        "        unigrams: list containing (unigram, probability) tuples\n",
        "        bigrams: list containing (bigram, probability) tuples\n",
        "        trigrams: list containing (trigram, probability) tuples\n",
        "    Output:\n",
        "        potential_tokens_from_unigrams: dictionary with key = position of the token saved as [sequence_index, token_index]\n",
        "            and value = all unigrams and their probabilites as (unigram, probability)\n",
        "        potential_tokens_from_bigrams: dictionary with key = position of the token saved as [sequence_index, token_index]\n",
        "            and value = all possible bigrams, given the context of the token, and their probabilities as (bigram, probability)\n",
        "        potential_tokens_from_trigrams: dictionary with key = position of the token saved as [sequence_index, token_index]\n",
        "            and value = all possible trigrams, given the context of the token, and their probabilities as (trigram, probability)\n",
        "    Example:\n",
        "        Input:\n",
        "            trigram: (ias satr aiftir, 0.0256)\n",
        "            sequence 5, position 7: ('ias', '<com>')\n",
        "            sequence 5, position 8: ('s--r', '<inc>')\n",
        "            sequence 5, position 9: ('aiftir', '<com>')\n",
        "        Output:\n",
        "            potential_tokens_from_trigrams[5, 8] = [(satr, 0.0256)]\n",
        "    \"\"\"\n",
        "    potential_tokens_from_unigrams = defaultdict(list)\n",
        "    potential_tokens_from_bigrams = defaultdict(list)\n",
        "    potential_tokens_from_trigrams = defaultdict(list)\n",
        "\n",
        "    total_length = len(unigrams) + len(bigrams) + len(trigrams)\n",
        "    pbar = tqdm(total=total_length, desc=\"Extracting potential tokens\", ncols=80)\n",
        "\n",
        "    # For each trigram, the code slides over the sequences with a window of size three tokens\n",
        "    for trigram, probability in trigrams:\n",
        "        pbar.update()\n",
        "\n",
        "        for i, sequence in enumerate(tagged_sequences):\n",
        "            for j in range(len(sequence) - 2):\n",
        "                # The code checks whether the sequence contains two <com> tags (tokens used as reference) and one <inc> tag (token to predict)\n",
        "                # Additionally, the code checks whether all tokens with a <com> tag in the window with position j also occur in the trigram at position j\n",
        "                if (\n",
        "                    all(\n",
        "                        (\n",
        "                            trigram[k] == sequence[j + k][0]\n",
        "                            if sequence[j + k][1] == \"<com>\"\n",
        "                            else True\n",
        "                        )\n",
        "                        for k in range(3)\n",
        "                    )\n",
        "                    and sum(sequence[j + k][1] == \"<com>\" for k in range(3)) == 2\n",
        "                ):\n",
        "                    for k in range(3):\n",
        "                        if sequence[j + k][1] == \"<inc>\":\n",
        "                            # If both conditions are met, the potential_token dictionary with key = position of an <inc> token is appended with the relevant\n",
        "                            # candidate from the trigram\n",
        "                            potential_tokens_from_trigrams[(i, j + k)].append(\n",
        "                                (trigram[k], probability)\n",
        "                            )\n",
        "\n",
        "    # For each bigram, the code slides over the sequences with a window of size two tokens\n",
        "    for bigram, probability in bigrams:\n",
        "        pbar.update()\n",
        "\n",
        "        for i, sequence in enumerate(tagged_sequences):\n",
        "            for j in range(len(sequence) - 1):\n",
        "                # The code checks whether the sequence contains one <com> tag (token used as reference) and one <inc> tag (token to predict)\n",
        "                # Additionally, the code checks whether all tokens with a <com> tag in the window with position j also occur in the bigram at position j\n",
        "                if (\n",
        "                    all(\n",
        "                        (\n",
        "                            bigram[k] == sequence[j + k][0]\n",
        "                            if sequence[j + k][1] == \"<com>\"\n",
        "                            else True\n",
        "                        )\n",
        "                        for k in range(2)\n",
        "                    )\n",
        "                    and sum(sequence[j + k][1] == \"<com>\" for k in range(2)) == 1\n",
        "                ):\n",
        "                    for k in range(2):\n",
        "                        if sequence[j + k][1] == \"<inc>\":\n",
        "                            # If both conditions are met, the potential_token dictionary with key = position of an <inc> token is appended with\n",
        "                            # the relevant candidate from the bigram\n",
        "                            potential_tokens_from_bigrams[(i, j + k)].append(\n",
        "                                (bigram[k], probability)\n",
        "                            )\n",
        "\n",
        "    # For unigrams, all unigrams and their probabilities are appended to potential_tokens_from_unigrams with key = position of an <inc> token\n",
        "    for unigram, probability in unigrams:\n",
        "        pbar.update()\n",
        "\n",
        "        for i, sequence in enumerate(tagged_sequences):\n",
        "            for j in range(len(sequence)):\n",
        "                if sequence[j][1] == \"<inc>\":\n",
        "                    potential_tokens_from_unigrams[(i, j)].append(\n",
        "                        (unigram, probability)\n",
        "                    )\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    return (\n",
        "        dict(potential_tokens_from_unigrams),\n",
        "        dict(potential_tokens_from_bigrams),\n",
        "        dict(potential_tokens_from_trigrams),\n",
        "    )\n",
        "\n",
        "\n",
        "def get_best_candidates(\n",
        "    sequences,\n",
        "    bigram_candidates_dict,\n",
        "    trigram_candidates_dict,\n",
        "    unigram_candidates_dict=None,\n",
        "    maximum_score=0,\n",
        "    number_predictions=float(\"inf\"),\n",
        "):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        sequences: list containing runic sequences as (token, tag) tuples\n",
        "        bigram_candidates_dict: dictionary with key = position of the token and value = all possible bigrams in that position\n",
        "        trigram_candidates_dict: dictionary with key = position of the token and value = all possible trigrams in that position\n",
        "        unigram_candidates_dict: (optional variable) dictionary with key = position of the token and value = all possible unigrams\n",
        "        maximum_score: the maximum allowed modified Minimum Edit Distance between the actual token and the candidate token\n",
        "            e.g. token = r-sa, risa (MED = 0), raisa (MED = 1)\n",
        "            0 = characters can only be added in positions where characters are known to be missing\n",
        "            higher MED = higher recall, lower accuracy\n",
        "        number_predictions: the maximum number of predictions that the function may return\n",
        "    Output: best_candidates: dictionary with key = position of the token and value = top candidates for that position, ordered as follows:\n",
        "            1. Trigram candidates > Bigram candidates (> Unigram candidates)\n",
        "            2. Highest to lowest n-gram probability\n",
        "    \"\"\"\n",
        "    best_candidates = {}\n",
        "\n",
        "    total_length = sum(\n",
        "        sum(1 for token_tag in sequence if token_tag[1] == \"<inc>\")\n",
        "        for sequence in sequences\n",
        "    )\n",
        "    pbar = tqdm(total=total_length, desc=\"Extracting best candidates\", ncols=80)\n",
        "\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        for j, token_tag in enumerate(sequence):\n",
        "            if sequence[j][1] == \"<inc>\":\n",
        "                pbar.update()\n",
        "                max_score_unigrams = []\n",
        "                max_score_bigrams = []\n",
        "                max_score_trigrams = []\n",
        "\n",
        "                if unigram_candidates_dict and (i, j) in unigram_candidates_dict.keys():\n",
        "                    for candidate, probability in unigram_candidates_dict[i, j]:\n",
        "                        MED_score = min_edit_distance(token_tag[0], candidate)\n",
        "                        if MED_score <= maximum_score:\n",
        "                            max_score_unigrams.append(\n",
        "                                (candidate, MED_score, probability)\n",
        "                            )\n",
        "\n",
        "                if (i, j) in bigram_candidates_dict.keys():\n",
        "                    for candidate, probability in bigram_candidates_dict[i, j]:\n",
        "                        MED_score = min_edit_distance(token_tag[0], candidate)\n",
        "                        if MED_score <= maximum_score:\n",
        "                            max_score_bigrams.append(\n",
        "                                (candidate, MED_score, probability)\n",
        "                            )\n",
        "\n",
        "                if (i, j) in trigram_candidates_dict.keys():\n",
        "                    for candidate, probability in trigram_candidates_dict[i, j]:\n",
        "                        MED_score = min_edit_distance(token_tag[0], candidate)\n",
        "                        if MED_score <= maximum_score:\n",
        "                            max_score_trigrams.append(\n",
        "                                (candidate, MED_score, probability)\n",
        "                            )\n",
        "\n",
        "                max_score_unigrams = list(set(max_score_unigrams))\n",
        "                max_score_unigrams = sorted(\n",
        "                    max_score_unigrams, key=lambda x: (x[1], -x[2])\n",
        "                )\n",
        "                max_score_bigrams = list(set(max_score_bigrams))\n",
        "                max_score_bigrams = sorted(\n",
        "                    max_score_bigrams, key=lambda x: (x[1], -x[2])\n",
        "                )\n",
        "                max_score_trigrams = list(set(max_score_trigrams))\n",
        "                max_score_trigrams = sorted(\n",
        "                    max_score_trigrams, key=lambda x: (x[1], -x[2])\n",
        "                )\n",
        "\n",
        "                potential_candidates_trigrams = [c[0] for c in max_score_trigrams]\n",
        "                potential_candidates_bigrams = [c[0] for c in max_score_bigrams]\n",
        "                potential_candidates_unigrams = [c[0] for c in max_score_unigrams]\n",
        "\n",
        "                merged_candidates = (\n",
        "                    potential_candidates_trigrams\n",
        "                    + [\n",
        "                        candidate\n",
        "                        for candidate in potential_candidates_bigrams\n",
        "                        if candidate not in potential_candidates_trigrams\n",
        "                    ]\n",
        "                    + [\n",
        "                        candidate\n",
        "                        for candidate in potential_candidates_unigrams\n",
        "                        if candidate not in potential_candidates_bigrams\n",
        "                        and candidate not in potential_candidates_trigrams\n",
        "                    ]\n",
        "                )\n",
        "                best_candidates[i, j] = merged_candidates[:number_predictions]\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    return best_candidates\n",
        "\n",
        "\n",
        "def min_edit_distance(source, target):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        source: source token containing missing characters\n",
        "            '-' single missing character\n",
        "            '…' multiple missing characters\n",
        "        target: potential candidate token\n",
        "    Output:\n",
        "        distance_matrix[-1][-1]: the bottem right value in the distance_matrix representing the final Minimum Edit Distance\n",
        "    Modifications:\n",
        "        for each '-' symbol, the function allows one free substitutions\n",
        "        for each '…' symbol, the function allows multiple free insertions\n",
        "    \"\"\"\n",
        "    distance_matrix = np.zeros((len(source) + 1, len(target) + 1))\n",
        "\n",
        "    for i in range(len(source) + 1):\n",
        "        distance_matrix[i][0] = i\n",
        "    for j in range(len(target) + 1):\n",
        "        distance_matrix[0][j] = j\n",
        "\n",
        "    for i in range(1, len(source) + 1):\n",
        "        for j in range(1, len(target) + 1):\n",
        "            if source[i - 1] == target[j - 1]:\n",
        "                distance_matrix[i][j] = distance_matrix[i - 1][j - 1]\n",
        "\n",
        "            elif source[i - 1] == \"-\":\n",
        "                distance_matrix[i][j] = distance_matrix[i - 1][j - 1]\n",
        "\n",
        "            # By allowing the model to also choose from the value on its left, it can make infinite free insertions for the '…' token\n",
        "            elif source[i - 1] == \"…\":\n",
        "                distance_matrix[i][j] = min(\n",
        "                    distance_matrix[i - 1][j - 1], distance_matrix[i][j - 1]\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                distance_matrix[i][j] = min(\n",
        "                    distance_matrix[i - 1][j - 1] + 2,\n",
        "                    distance_matrix[i - 1][j] + 1,\n",
        "                    distance_matrix[i][j - 1] + 1,\n",
        "                )\n",
        "\n",
        "    return distance_matrix[-1][-1]\n",
        "\n",
        "\n",
        "def integrate_candidates(sequences, best_candidates, k):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        sequences: list containing runic sequences with (incomplete token, <inc> tag) tuples\n",
        "        best_candidates: dictionary with key = position of the token and value = top candidates for that position\n",
        "        k: number of candidates are considered from the best_candidates\n",
        "            (this option is incorporated for efficient testing of model as it only requires the candidates to be calculated once)\n",
        "    Output:\n",
        "        sequences: list containing runic sequences with (list of complete candidates, <mod> tag) tuples\n",
        "    \"\"\"\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        for j, _ in enumerate(sequence):\n",
        "            if (i, j) in best_candidates.keys():\n",
        "                sequence[j] = (best_candidates[(i, j)][:k], \"<mod>\")\n",
        "\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def extract_test_samples(test_set):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_set: list containing the test sequences as (token, tag) tuples\n",
        "    Output:\n",
        "        test_set: list with selection of viable sequences as (token, tag) tuples\n",
        "    \"\"\"\n",
        "    for i, sequence in enumerate(test_set):\n",
        "        longest_sequence = []\n",
        "        current_sequence = []\n",
        "        for token, tag in sequence:\n",
        "            if tag == \"<com>\":\n",
        "                current_sequence.append(token)\n",
        "                if len(current_sequence) > len(longest_sequence):\n",
        "                    longest_sequence = current_sequence\n",
        "        else:\n",
        "            current_sequence = []\n",
        "\n",
        "        if len(longest_sequence) >= 4:\n",
        "            test_set[i] = longest_sequence\n",
        "\n",
        "        else:\n",
        "            test_set[i] = []\n",
        "\n",
        "    test_set = [sequence for sequence in test_set if sequence]\n",
        "\n",
        "    return test_set\n",
        "\n",
        "\n",
        "def alter_token(token, seed):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        token: a token from which a number of characters need to be replaced by '-' and/or '…'\n",
        "        seed: an integer between 0 and 4\n",
        "    Output:\n",
        "        Modified token with one or multiple characters replaced by '-' and/or '…'\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    token = list(token)\n",
        "    max_changes = len(token) - 1\n",
        "    number_of_changes = 0\n",
        "\n",
        "    replacement = random.choice([\"-\", \"…\"])\n",
        "\n",
        "    if replacement == \"-\":\n",
        "        while number_of_changes != max_changes:\n",
        "            pos = random.randint(0, len(token) - 1)\n",
        "            token[pos] = \"-\"\n",
        "            number_of_changes += 1\n",
        "            add_changes = random.choice([\"yes\", \"no\"])\n",
        "            if add_changes == \"no\":\n",
        "                break\n",
        "        return \"\".join(token)\n",
        "\n",
        "    if replacement == \"…\":\n",
        "        start = random.randint(0, len(token) - 2)\n",
        "        end = random.randint(start + 1, len(token) - 1)\n",
        "        return \"\".join(token[:start]) + replacement + \"\".join(token[end:])\n",
        "\n",
        "\n",
        "def alter_sequence(sequence, seed):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        sequence: list containing a single test sequence as (token, <com> tag) tuples\n",
        "        seed: an integer between 0 and 4\n",
        "    Output:\n",
        "        sequence: list containing a single test sequence with (modified_token, <inc> tag) tuples\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    viable_tokens = []\n",
        "    # Create a copy of the sequence\n",
        "    sequence = sequence[:]\n",
        "    for index, token in enumerate(sequence):\n",
        "        if len(token) > 1:\n",
        "            viable_tokens.append(index)\n",
        "\n",
        "    if len(sequence) in [4]:\n",
        "        num_tokens_to_alter = 1\n",
        "    elif len(sequence) in [5, 6]:\n",
        "        num_tokens_to_alter = 2\n",
        "    else:\n",
        "        num_tokens_to_alter = 3\n",
        "\n",
        "    tokens_to_alter = random.sample(\n",
        "        viable_tokens, min(num_tokens_to_alter, len(viable_tokens))\n",
        "    )\n",
        "\n",
        "    for i in tokens_to_alter:\n",
        "        sequence[i] = alter_token(sequence[i], seed)\n",
        "\n",
        "    return sequence\n",
        "\n",
        "\n",
        "def alter_sequences(sequences):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        sequences: list containing all test sequences as (token, <com> tag) tuples\n",
        "    Output:\n",
        "        altered_sequences: list containing all test sequences with (modified_token, <inc> tag) tuples\n",
        "    \"\"\"\n",
        "    altered_sequences = []\n",
        "    for sequence in sequences:\n",
        "        for seed in range(5):\n",
        "            altered_sequence = alter_sequence(sequence, seed)\n",
        "            altered_sequences.append(altered_sequence)\n",
        "    return altered_sequences\n",
        "\n",
        "\n",
        "def calculate_F_score(gold_pred_list):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        gold_pred_list: list containing tuples as (gold standard, [list of predictions])\n",
        "    Output:\n",
        "        Metrics: recall, precision and F_score\n",
        "        Raw data: correct_predictions, incorrect_predictions and non_predictions\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    incorrect_predictions = 0\n",
        "    non_predictions = 0\n",
        "\n",
        "    for item in gold_pred_list:\n",
        "        gold, pred = item\n",
        "        for i, predictions in enumerate(pred):\n",
        "            if predictions[1] == \"<mod>\":\n",
        "                if len(predictions[0]) == 0:\n",
        "                    non_predictions += 1\n",
        "                else:\n",
        "                    if gold[i] in predictions[0]:\n",
        "                        correct_predictions += 1\n",
        "                    else:\n",
        "                        incorrect_predictions += 1\n",
        "\n",
        "    predicted = correct_predictions + incorrect_predictions\n",
        "    to_predict = correct_predictions + incorrect_predictions + non_predictions\n",
        "\n",
        "    recall = (to_predict - non_predictions) / to_predict\n",
        "    precision = correct_predictions / predicted\n",
        "\n",
        "    F_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return (\n",
        "        recall,\n",
        "        precision,\n",
        "        F_score,\n",
        "        correct_predictions,\n",
        "        incorrect_predictions,\n",
        "        non_predictions,\n",
        "    )\n",
        "\n",
        "def make_predictions(path, max_score, k):\n",
        "\n",
        "    # Read in the runestones\n",
        "    runestones = read_runestones(path)\n",
        "\n",
        "    # Write the runestones to a dataframe for pre-processing\n",
        "    df_runestones = pd.DataFrame(runestones)\n",
        "\n",
        "    # Apply pre-processing\n",
        "    punct = print_non_alpha(runestones)\n",
        "    df_runestones.iloc[:, 0] = df_runestones.iloc[:, 0].apply(lambda x: remove_punctuation(x, punct))\n",
        "    df_runestones.iloc[:, 0] = df_runestones.iloc[:, 0].apply(tokenize_text)\n",
        "    df_runestones.iloc[:, 0] = df_runestones.iloc[:, 0].apply(get_tags)\n",
        "    df_runestones = df_runestones[df_runestones.iloc[:, 0].apply(lambda x: len(x) >= 3)]\n",
        "    df_runestones.to_csv(\"Processed_Runestones.csv\", index=False)\n",
        "\n",
        "    runestones = df_runestones.iloc[:, 0].tolist()\n",
        "\n",
        "    # Load unigram tokens\n",
        "    with open(\"unigram_tokens.pkl\", \"rb\") as f:\n",
        "        extracted_unigram_tokens = pickle.load(f)\n",
        "\n",
        "    # Load bigram tokens\n",
        "    with open(\"bigram_tokens.pkl\", \"rb\") as f:\n",
        "        extracted_bigram_tokens = pickle.load(f)\n",
        "\n",
        "    # Load trigram tokens\n",
        "    with open(\"trigram_tokens.pkl\", \"rb\") as f:\n",
        "        extracted_trigram_tokens = pickle.load(f)\n",
        "\n",
        "    unigram_candidates_dict, bigram_candidates_dict, trigram_candidates_dict = (\n",
        "        extract_potential_tokens(\n",
        "            runestones,\n",
        "            extracted_unigram_tokens,\n",
        "            extracted_bigram_tokens,\n",
        "            extracted_trigram_tokens,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    best_candidates = get_best_candidates(\n",
        "        runestones,\n",
        "        bigram_candidates_dict,\n",
        "        trigram_candidates_dict,\n",
        "        unigram_candidates_dict=unigram_candidates_dict,\n",
        "        maximum_score=max_score,\n",
        "        number_predictions=k,\n",
        "    )\n",
        "\n",
        "    original = copy.deepcopy(runestones)\n",
        "    predictions = integrate_candidates(runestones, best_candidates, k)\n",
        "    zipped = zip(original, predictions)\n",
        "    original_pred = [(original, prediction) for original, prediction in zipped]\n",
        "\n",
        "    print()\n",
        "\n",
        "    for i in range(len(predictions)):\n",
        "      print(f\"\\nOriginal sentence:\\t{original_pred[i][0]}\")\n",
        "      print(f\"Modified sentence:\\t{original_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load and process runic sequences**\n",
        "\n",
        "* Write the runic sequences to a .txt file with one runic inscription per line.\n",
        "* A single missing character must be denoted using the '-' symbol.\n",
        "* Multiple missing characters must be denoted using the '…' symbol.\n",
        "\n",
        "`\"/content/runic_inscriptions.txt\"` is an example path, you can change this by the actual path for your data\n",
        "\n",
        "`max_score` determines the maximum edit distance between your input and a candidate. If you are certain that your input is accurate, keep this value at 0. If it is possible that certain characters have been incorrectly transliterated, you could increase this value slightly (every 1 point allows a character to be removed or added).\n",
        "\n",
        "`k` determines the maximum number of predictions that are returned per incomplete token. Improving `k`can improve accuracies slightly, with `k=10` providing an accuracy of *81.58%* and `k=100` providing an accuracy of *86.75%*"
      ],
      "metadata": {
        "id": "ATMBGkHQT3bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_predictions(\"/content/runic_inscriptions.txt\", max_score=0, k=20)"
      ],
      "metadata": {
        "id": "0oyV4qWqy649"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}